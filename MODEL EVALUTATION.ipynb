{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<H1> RANDOM FOREST CLASSIFIER"
      ],
      "metadata": {
        "id": "GuEEhtEuYWGQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUEEI7JefUbn",
        "outputId": "ed78d913-de4a-413a-c7f6-dfeb18a5c6b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.64      0.81      0.71        93\n",
            "        True       0.78      0.61      0.68       107\n",
            "\n",
            "    accuracy                           0.70       200\n",
            "   macro avg       0.71      0.71      0.70       200\n",
            "weighted avg       0.72      0.70      0.70       200\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"toxic comments.csv\")\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "X = data['Text']\n",
        "y = data['IsToxic']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature extraction using TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "# Classification report\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<H1>GRID SEARCH"
      ],
      "metadata": {
        "id": "KhnD_vSVYdpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"toxic comments.csv\")\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "X = data['Text']\n",
        "y = data['IsToxic']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature extraction using TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=10000, stop_words='english', ngram_range=(1, 2))\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Parameter grid for grid search\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Instantiate the grid search model\n",
        "grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, n_jobs=-1)\n",
        "\n",
        "# Train the model using the grid search\n",
        "grid_search.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Best parameters found by grid search\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best Parameters:\", best_params)\n",
        "\n",
        "# Predictions\n",
        "y_pred = grid_search.predict(X_test_tfidf)\n",
        "\n",
        "# Classification report\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_I1x3v_1f3kZ",
        "outputId": "57d576a4-e03c-4fde-957a-5f5d7f16d664"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.62      0.87      0.72        93\n",
            "        True       0.83      0.53      0.65       107\n",
            "\n",
            "    accuracy                           0.69       200\n",
            "   macro avg       0.72      0.70      0.69       200\n",
            "weighted avg       0.73      0.69      0.68       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<H1> GRADIENT BOOSTING CLASSIFIER"
      ],
      "metadata": {
        "id": "pgF5PcUCYhFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"toxic comments.csv\")\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "X = data['Text']\n",
        "y = data['IsToxic']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature extraction using TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=10000, stop_words='english', ngram_range=(1, 2))\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Train a Gradient Boosting Classifier\n",
        "model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "# Classification report\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Qh7iIiof3ny",
        "outputId": "99d8e0c9-ff61-4c12-8ae8-c349facc3ba8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.61      0.83      0.70        93\n",
            "        True       0.78      0.53      0.63       107\n",
            "\n",
            "    accuracy                           0.67       200\n",
            "   macro avg       0.69      0.68      0.67       200\n",
            "weighted avg       0.70      0.67      0.66       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"toxic comments.csv\")\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "X = data['Text']\n",
        "y = data['IsToxic']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature extraction using TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=10000, stop_words='english', ngram_range=(1, 2))\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Train a Gradient Boosting Classifier with optimized parameters\n",
        "model = GradientBoostingClassifier(n_estimators=300, learning_rate=0.05, max_depth=5, min_samples_split=5, random_state=42)\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "# Classification report\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67wvj7bbiT_i",
        "outputId": "fff89820-08a7-4601-923d-2244c536d407"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.64      0.82      0.72        93\n",
            "        True       0.79      0.60      0.68       107\n",
            "\n",
            "    accuracy                           0.70       200\n",
            "   macro avg       0.71      0.71      0.70       200\n",
            "weighted avg       0.72      0.70      0.70       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PbtfN5ibYoZE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<H1> CNN\n"
      ],
      "metadata": {
        "id": "xMgBHxcTY2mB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"toxic comments.csv\")\n",
        "\n",
        "# Splitting the data into training, validation, and testing sets\n",
        "X = data['Text']\n",
        "y = data['IsToxic']\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature extraction using TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=10000, stop_words='english', ngram_range=(1, 2))\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_val_tfidf = tfidf_vectorizer.transform(X_val)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Reshape TF-IDF matrices for compatibility with CNN\n",
        "max_features = 10000  # number of features extracted by TF-IDF\n",
        "sequence_length = X_train_tfidf.shape[1]  # length of each input sequence\n",
        "X_train_cnn = X_train_tfidf.toarray().reshape(-1, sequence_length, 1)\n",
        "X_val_cnn = X_val_tfidf.toarray().reshape(-1, sequence_length, 1)\n",
        "X_test_cnn = X_test_tfidf.toarray().reshape(-1, sequence_length, 1)\n",
        "\n",
        "# Define the CNN model\n",
        "model = Sequential([\n",
        "    Conv1D(128, 5, activation='relu', input_shape=(sequence_length, 1)),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Early stopping to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_cnn, y_train, validation_data=(X_val_cnn, y_val), epochs=20, batch_size=32, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test_cnn, y_test)\n",
        "print(f'Test Accuracy: {accuracy:.2f}')\n",
        "\n",
        "# Predictions\n",
        "y_pred_prob = model.predict(X_test_cnn)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "# Classification report\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuwx9inia9Jd",
        "outputId": "de443ffc-0728-4cc4-bc0b-7904592d32e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "20/20 [==============================] - 3s 47ms/step - loss: 0.6913 - accuracy: 0.5578 - val_loss: 0.6889 - val_accuracy: 0.6000\n",
            "Epoch 2/20\n",
            "20/20 [==============================] - 1s 33ms/step - loss: 0.6902 - accuracy: 0.5469 - val_loss: 0.6860 - val_accuracy: 0.6000\n",
            "Epoch 3/20\n",
            "20/20 [==============================] - 1s 28ms/step - loss: 0.6896 - accuracy: 0.5453 - val_loss: 0.6841 - val_accuracy: 0.6000\n",
            "Epoch 4/20\n",
            "20/20 [==============================] - 1s 26ms/step - loss: 0.6892 - accuracy: 0.5484 - val_loss: 0.6828 - val_accuracy: 0.6000\n",
            "Epoch 5/20\n",
            "20/20 [==============================] - 1s 26ms/step - loss: 0.6890 - accuracy: 0.5469 - val_loss: 0.6816 - val_accuracy: 0.6000\n",
            "Epoch 6/20\n",
            "20/20 [==============================] - 1s 25ms/step - loss: 0.6881 - accuracy: 0.5453 - val_loss: 0.6807 - val_accuracy: 0.6000\n",
            "Epoch 7/20\n",
            "20/20 [==============================] - 1s 26ms/step - loss: 0.6869 - accuracy: 0.5453 - val_loss: 0.6797 - val_accuracy: 0.6000\n",
            "Epoch 8/20\n",
            "20/20 [==============================] - 1s 26ms/step - loss: 0.6879 - accuracy: 0.5453 - val_loss: 0.6787 - val_accuracy: 0.6000\n",
            "Epoch 9/20\n",
            "20/20 [==============================] - 1s 26ms/step - loss: 0.6886 - accuracy: 0.5453 - val_loss: 0.6778 - val_accuracy: 0.6000\n",
            "Epoch 10/20\n",
            "20/20 [==============================] - 1s 27ms/step - loss: 0.6886 - accuracy: 0.5453 - val_loss: 0.6775 - val_accuracy: 0.6000\n",
            "Epoch 11/20\n",
            "20/20 [==============================] - 1s 27ms/step - loss: 0.6875 - accuracy: 0.5453 - val_loss: 0.6776 - val_accuracy: 0.6000\n",
            "Epoch 12/20\n",
            "20/20 [==============================] - 1s 25ms/step - loss: 0.6882 - accuracy: 0.5453 - val_loss: 0.6771 - val_accuracy: 0.6000\n",
            "Epoch 13/20\n",
            "20/20 [==============================] - 1s 25ms/step - loss: 0.6874 - accuracy: 0.5453 - val_loss: 0.6768 - val_accuracy: 0.6000\n",
            "Epoch 14/20\n",
            "20/20 [==============================] - 1s 28ms/step - loss: 0.6864 - accuracy: 0.5453 - val_loss: 0.6767 - val_accuracy: 0.6000\n",
            "Epoch 15/20\n",
            "20/20 [==============================] - 1s 30ms/step - loss: 0.6878 - accuracy: 0.5453 - val_loss: 0.6762 - val_accuracy: 0.6000\n",
            "Epoch 16/20\n",
            "20/20 [==============================] - 1s 30ms/step - loss: 0.6862 - accuracy: 0.5453 - val_loss: 0.6761 - val_accuracy: 0.6000\n",
            "Epoch 17/20\n",
            "20/20 [==============================] - 1s 29ms/step - loss: 0.6871 - accuracy: 0.5453 - val_loss: 0.6759 - val_accuracy: 0.6000\n",
            "Epoch 18/20\n",
            "20/20 [==============================] - 1s 26ms/step - loss: 0.6862 - accuracy: 0.5453 - val_loss: 0.6753 - val_accuracy: 0.6000\n",
            "Epoch 19/20\n",
            "20/20 [==============================] - 1s 26ms/step - loss: 0.6859 - accuracy: 0.5453 - val_loss: 0.6754 - val_accuracy: 0.6000\n",
            "Epoch 20/20\n",
            "20/20 [==============================] - 1s 26ms/step - loss: 0.6873 - accuracy: 0.5453 - val_loss: 0.6756 - val_accuracy: 0.6000\n",
            "7/7 [==============================] - 0s 12ms/step - loss: 0.7096 - accuracy: 0.4650\n",
            "Test Accuracy: 0.47\n",
            "7/7 [==============================] - 0s 10ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.47      1.00      0.63        93\n",
            "           1       0.00      0.00      0.00       107\n",
            "\n",
            "    accuracy                           0.47       200\n",
            "   macro avg       0.23      0.50      0.32       200\n",
            "weighted avg       0.22      0.47      0.30       200\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Calculate class weights\n",
        "class_weights = {0: 1.0, 1: np.sum(y_train == 0) / np.sum(y_train == 1)}\n",
        "\n",
        "# Train the model with class weights\n",
        "history = model.fit(X_train_cnn, y_train, validation_data=(X_val_cnn, y_val), epochs=20, batch_size=32, callbacks=[early_stopping], class_weight=class_weights)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDaLK82Cbkeh",
        "outputId": "1d8607c1-917a-4c30-f38f-8f645d2bcd29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "20/20 [==============================] - 1s 48ms/step - loss: 0.7585 - accuracy: 0.5453 - val_loss: 0.6759 - val_accuracy: 0.6000\n",
            "Epoch 2/20\n",
            "20/20 [==============================] - 1s 29ms/step - loss: 0.7579 - accuracy: 0.5453 - val_loss: 0.6779 - val_accuracy: 0.6000\n",
            "Epoch 3/20\n",
            "20/20 [==============================] - 1s 30ms/step - loss: 0.7570 - accuracy: 0.5453 - val_loss: 0.6792 - val_accuracy: 0.6000\n",
            "Epoch 4/20\n",
            "20/20 [==============================] - 1s 31ms/step - loss: 0.7547 - accuracy: 0.5453 - val_loss: 0.6801 - val_accuracy: 0.6000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BERT**\n"
      ],
      "metadata": {
        "id": "E1I13KCOdr31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"toxic comments.csv\")\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "X = data['Text']\n",
        "y = data['IsToxic']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize the input text and convert to input IDs\n",
        "def tokenize_text(text):\n",
        "    return tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=128,  # BERT supports up to 512 tokens\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "\n",
        "# Tokenize the training and testing data\n",
        "X_train_tokenized = [tokenize_text(text) for text in X_train]\n",
        "X_test_tokenized = [tokenize_text(text) for text in X_test]\n",
        "\n",
        "# Convert lists of dictionaries to dictionaries of tensors\n",
        "X_train_input = {\n",
        "    'input_ids': tf.concat([x['input_ids'] for x in X_train_tokenized], axis=0),\n",
        "    'attention_mask': tf.concat([x['attention_mask'] for x in X_train_tokenized], axis=0)\n",
        "}\n",
        "X_test_input = {\n",
        "    'input_ids': tf.concat([x['input_ids'] for x in X_test_tokenized], axis=0),\n",
        "    'attention_mask': tf.concat([x['attention_mask'] for x in X_test_tokenized], axis=0)\n",
        "}\n",
        "\n",
        "# Convert labels to tensors\n",
        "y_train = tf.convert_to_tensor(y_train.values)\n",
        "y_test = tf.convert_to_tensor(y_test.values)\n",
        "\n",
        "# Load pre-trained BERT model for sequence classification\n",
        "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train_input,\n",
        "    y_train,\n",
        "    validation_data=(X_test_input, y_test),\n",
        "    epochs=3,  # You can adjust the number of epochs\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test_input, y_test)\n",
        "print(f'Test Accuracy: {accuracy:.2f}')\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test_input)\n",
        "y_pred_labels = np.argmax(y_pred.logits, axis=1)\n",
        "\n",
        "# Classification report\n",
        "print(classification_report(y_test.numpy(), y_pred_labels))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcuwfiIudphK",
        "outputId": "b068a5e1-701a-407f-f405-c8d9f7cb4889"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function infer_framework at 0x7f1d41f9cee0> and will run it as-is.\n",
            "Cause: for/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: AutoGraph could not transform <function infer_framework at 0x7f1d41f9cee0> and will run it as-is.\n",
            "Cause: for/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "25/25 [==============================] - 114s 1s/step - loss: 1.2738 - accuracy: 0.5200 - val_loss: 0.6931 - val_accuracy: 0.4650\n",
            "Epoch 2/3\n",
            "25/25 [==============================] - 21s 841ms/step - loss: 0.6923 - accuracy: 0.4737 - val_loss: 0.6931 - val_accuracy: 0.4650\n",
            "Epoch 3/3\n",
            "25/25 [==============================] - 20s 806ms/step - loss: 0.7460 - accuracy: 0.4837 - val_loss: 0.6931 - val_accuracy: 0.5350\n",
            "7/7 [==============================] - 2s 238ms/step - loss: 0.6931 - accuracy: 0.5350\n",
            "Test Accuracy: 0.54\n",
            "7/7 [==============================] - 11s 232ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.00      0.00      0.00        93\n",
            "        True       0.54      1.00      0.70       107\n",
            "\n",
            "    accuracy                           0.54       200\n",
            "   macro avg       0.27      0.50      0.35       200\n",
            "weighted avg       0.29      0.54      0.37       200\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    }
  ]
}